version: "3.4"

x-mw-net: &mw-net
  networks:
    - mw

x-gis-net: &gis-net
  networks:
    - gis

x-app-net: &app-net
  networks:
    - mw
    - gis

x-admin-net: &admin-net
  networks:
    - admin

# Todo: move nvidia runtime to an override and make no-gpu the default?
# NVIDIA is default - this needs an override if your system does not have
# an NVIDIA GPU (e.g. Raspberry Pi 5 has Broadcom VideoCore)
x-gpu: &gpu
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities: [ gpu ]

# volumes conflict with x11-volumes-and-devices
x-ros-volumes: &ros-volumes
  volumes:
    - /dev/shm:/dev/shm

# volumes conflict with ros-volumes
x-x11-volumes-and-devices: &x11-volumes-and-devices
  environment: &x11-env
    QT_X11_NO_MITSHM: 1
    DISPLAY: ${DISPLAY}
  volumes:
    - /tmp/.X11-unix:/tmp/.X11-unix:ro
  devices:
    - /dev/dri:/dev/dri

# use this if both ros and x11 volumes and devices are needed
x-ros-x11-volumes-and-devices: &ros-x11-volumes-and-devices
  environment: *x11-env
  volumes:
    - /tmp/.X11-unix:/tmp/.X11-unix:ro  # x11
    - /dev/shm:/dev/shm  # ros
  devices:
    - /dev/dri:/dev/dri  # x11

x-labels: &labels
  org.gisnav.maintainer: "Harri Makelin <hmakelin@protonmail.com>"
  org.gisnav.website: "https://gisnav.org"

# the px4 service needs tty to make the mavlink/nsh shell work properly
x-tty: &tty
  stdin_open: true
  tty: true

volumes:
  maps-volume:
    labels:
      <<: *labels
      org.gisnav.description: "Volume for storing map data, exposed to end-user tools in the Docker host network"
  gscam-volume:
    labels:
      <<: *labels
      org.gisnav.description: "Volume for GSCam configuration data, exposed to end-user tools in the Docker host network"
  gisnav-volume:
    labels:
      <<: *labels
      org.gisnav.description: "Volume for GISNav ROS configuration such as launch and parameter files, exposed to end-user tools in the Docker host network"

networks:
  gis:
    driver: bridge
    attachable: true
    labels:
      <<: *labels
      org.gisnav.description: "Network for GIS related services"
  mw:
    driver: bridge
    attachable: true
    labels:
      <<: *labels
      org.gisnav.description: "Network for autopilot middleware communication"
  admin:
    driver: bridge
    attachable: true
    labels:
      <<: *labels
      org.gisnav.description: "Network for administrative access and management, where end-user tools can access specific volumes that contain configuration data"

services:
  mapserver:
    <<: [ *gis-net ]
    build:
      context: apache
      target: mapserver
    command: apache2ctl -D FOREGROUND
    volumes:
      - maps-volume:/etc/mapserver/maps
    labels:
      <<: *labels
      homepage.group: GIS services
      homepage.name: MapServer
      homepage.description: MapServer GIS service

  mavros:
    <<: [ *ros-volumes, *mw-net ]
    build:
      context: mavros
      target: mavros
      args:
        ROS_VERSION: humble
    command: ros2 run mavros mavros_node --ros-args --param fcu_url:=udp://:14540@localhost:14557
    labels:
      <<: *labels
      homepage.group: Middleware services
      homepage.name: MAVROS
      homepage.description: MAVLink to ROS middleware

  micro-ros-agent:
    <<: [ *ros-volumes, *mw-net ]
    build:
      context: micro-ros-agent
    command: udp4 -p 8888
    labels:
      <<: *labels
      homepage.group: Middleware services
      homepage.name: micro-ROS agent
      homepage.description: uORB to ROS (PX4) middleware

  qgc:
    <<: [ *x11-volumes-and-devices, *mw-net ]
    build:
      context: qgc
    volumes:
      - /tmp/.X11-unix:/tmp/.X11-unix:ro
      - /dev/shm:/dev/shm
      - /dev/dri:/dev/dri
    privileged: true
    labels:
      <<: *labels
      homepage.group: Ground services
      homepage.name: QGroundControl
      homepage.description: Ground control software with GUI for controlling the vehicle

  # Note: build context is repository root
  # Note 2: The "docker buildx build" command in the push_gisnav_images.yml workflow
  #  duplicates these build arguments. They should be changed together.
  # *x11 anchor is needed for debugging (cv2 visualization of matches) for this
  # service
  # Socat is used to bridge the output serial port to the PX4 SITL container
  # over TCP - mapping serial ports to Docker host serial ports would be
  # a bit more messy. For production deployments the socat command is expected
  # to be left out.
  gisnav:
    <<: [ *ros-x11-volumes-and-devices, *gpu, *app-net ]
    #image: "ghcr.io/hmakelin/gisnav:${TAG:-latest}"
    image: "${GISNAV_DOCKER_REGISTRY_HOST:-ghcr.io}:${GISNAV_DOCKER_REGISTRY_PORT:-443}/${GISNAV_DOCKER_REGISTRY_NAMESPACE}/gisnav"
    build:
      context: ..
      dockerfile: docker/mavros/Dockerfile
      target: gisnav
      args:
        ROS_VERSION: humble
    volumes:
      - /dev/shm:/dev/shm
      - /tmp/.X11-unix:/tmp/.X11-unix:ro
      - /dev/dri:/dev/dri  # TODO: make volumes merging work with ancors (x11, ros, etc?)
      - gisnav-volume:/etc/gisnav
    command: ros2 launch gisnav default.launch.py protocol:=uorb
    #command: socat pty,link=/dev/ttyS1 tcp:gisnav-px4-1:15000 & ros2 launch gisnav default.launch.py protocol:=nmea
    depends_on:
      - micro-ros-agent
      - mavros
      - gscam
      - mapserver
      - postgres
      - autoheal
      - nginx
    labels:
      <<: *labels
      homepage.group: Application services
      homepage.name: GISNav
      homepage.description: Generates mock GPS messages for FCU using visual map-based navigation

  # The px4 service depends on mavros, mapserver and micro-ros-agent because
  # their IP addresses are used in the px4 service entrypoint.sh script
  px4:
    <<: [ *x11-volumes-and-devices, *gpu, *tty, *mw-net ]
    build:
      context: px4
    volumes:
      - /tmp/.X11-unix:/tmp/.X11-unix:ro
      - /dev/shm:/dev/shm
      - /dev/dri:/dev/dri
    command: make px4_sitl gazebo-classic_typhoon_h480__ksql_airport
    depends_on:
      - mavros
      - micro-ros-agent
      - qgc
    labels:
      <<: *labels
      homepage.group: Simulation services
      homepage.name: PX4
      homepage.description: PX4 simulation and firmware tool

  ardupilot:
    <<: [ *x11-volumes-and-devices, *gpu, *mw-net ]
    build:
      context: ardupilot
    volumes:
      - /tmp/.X11-unix:/tmp/.X11-unix:ro
      - /dev/shm:/dev/shm
      - /dev/dri:/dev/dri
    privileged: True
    command: bash -c "cd ~ && make sim_vehicle"
    labels:
      <<: *labels
      homepage.group: Simulation services
      homepage.name: ArduPilot
      homepage.description: ArduPilot simulation and firmware tool

  rviz:
    <<: [ *x11-volumes-and-devices, *ros-volumes, *gpu, *mw-net ]
    build:
      context: rviz
      args:
        ROS_VERSION: humble
    command: rviz2 -d gisnav_config.rviz
    labels:
      <<: *labels
      homepage.group: Development services
      homepage.name: RViz
      homepage.description: ROS visualization tool

  gscam:
    <<: [ *ros-volumes, *mw-net ]
    build:
      context: gscam
      args:
        ROS_VERSION: humble
    volumes:
      - /dev/shm:/dev/shm  # todo: make work with *ros anchor
      - gscam-volume:/etc/gscam
    command:
      - "ros2"
      - "run"
      - "gscam"
      - "gscam_node"
      - "--ros-args"
      - "--params-file"
      - "/etc/gscam/gscam_params.yaml"
      - "-p"
      - "camera_info_url:=file:///etc/gscam/camera_calibration.yaml"
    labels:
      <<: *labels
      homepage.group: Middleware services
      homepage.name: gscam
      homepage.description: GStreamer camera middleware

  autoheal:
    <<: [ *app-net ]
    image: willfarrell/autoheal
    restart: always
    environment:
      AUTOHEAL_CONTAINER_LABEL: all
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    labels:
      <<: *labels
      homepage.group: Application services
      homepage.name: Autoheal
      homepage.description: Monitors and maintains health of other services.

  qgis:
    <<: [ *x11-volumes-and-devices, *gis-net ]
    build:
      context: qgis
    command: qgis
    depends_on:
      - postgres
      - mapserver
    labels:
      <<: *labels
      homepage.group: GIS services
      homepage.name: QGIS
      homepage.description: GIS client for viewing map rasters

  postgres:
    <<: [ *gis-net ]
    build:
      context: postgres
    environment:
      POSTGRES_DB: gisnav
      POSTGRES_USER: gisnav
      POSTGRES_PASSWORD: gisnav
    labels:
      <<: *labels
      homepage.group: Data services
      homepage.name: Postgres
      homepage.description: PostGIS relational database for efficiently storing geographical information

  fileserver:
    <<: [ *admin-net ]
    build:
      context: apache
      target: fileserver
    command: apache2ctl -D FOREGROUND
    volumes:
      - maps-volume:/var/www/filegator/repository/mapserver
      - gscam-volume:/var/www/filegator/repository/gscam
      - gisnav-volume:/var/www/filegator/repository/gisnav
    labels:
      <<: *labels
      homepage.group: Admin services
      homepage.name: FileGator
      homepage.description: File manager for uploading and removing orthoimagery and DEMs and editing ROS configuration files
      homepage.href: /fileserver
      homepage.target: _blank

  # Give this thing access to all networks to see docker container status?
  # extra_hosts with docker host needed for monitoring service
  homepage:
    <<: [ *admin-net ]
    build:
      context: homepage
    labels:
      <<: *labels
      homepage.group: Admin services
      homepage.name: Homepage
      homepage.description: Administration dashboard
      homepage.href: /
      homepage.target: _self
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro  # read-only for safety - admin dashboard uses this

  monitoring:
    <<: [ *gpu, *admin-net ]
    build:
      context: monitoring
    pid: host
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /run/user/1000/podman/podman.sock:/run/user/1000/podman/podman.sock
    command: glances -w
    labels:
      <<: *labels
      homepage.group: Admin services
      homepage.name: Glances
      homepage.description: System monitor
      homepage.href: /monitoring
      homepage.target: _blank

  nginx:
    <<: [ *admin-net ]
    image: nginx:latest
    container_name: nginx
    ports:
      - "80:80"  # Expose port 80 on the host to port 80 in the container in the admin network
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - fileserver
      - homepage
      - monitoring
    labels:
      <<: *labels
      homepage.group: Admin services
      homepage.name: Nginx
      homepage.description: Reverse proxy server
